{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ec4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from typing import Any, Callable, Dict, Optional\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import torch\n",
    "from ray import tune\n",
    "from sklearn import metrics\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdec6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d11a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    \"\"\"The class used for early stopping during training when the loss doesn't\n",
    "    decrease validly after some patience steps\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience: int = 1, min_delta: float = 0) -> None:\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        patience: int, default 1\n",
    "            the number of steps after which the training stops if the loss\n",
    "            doesn't decrease\n",
    "        min_delta: float, default 0\n",
    "            the minimal delta, if the current loss is more than the sum of the\n",
    "            delta and the minimal loss, the counter will be added 1 as one\n",
    "            non-decreasing iteration\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_loss = np.inf\n",
    "\n",
    "    def __call__(self, loss: float) -> bool:\n",
    "        \"\"\"Checks whether the non-valid non-decreasing loss is accumulated up to\n",
    "        the limit patience\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        loss: float\n",
    "            the current loss\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            the indicator if to stop the training\n",
    "        \"\"\"\n",
    "        if loss < self.min_loss:\n",
    "            # once there is a new minimal loss\n",
    "            self.min_loss = loss\n",
    "            self.counter = 0\n",
    "        elif loss > (self.min_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14888d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear network for regression\n",
    "class DNNNetRegressor(nn.Module):\n",
    "    \"\"\"The class to define a 3-layer linear Pytorch regression model\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        l1: int = 512,\n",
    "        l2: int = 128,\n",
    "        l3: int = 64,\n",
    "    ) -> None:\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: int\n",
    "            the input size which is the second dim from each batch\n",
    "        output_size: int\n",
    "            the final output dimension\n",
    "        l1: int, default 512\n",
    "            the number of output samples from the first layer\n",
    "        l2: int, default 128\n",
    "            the number of output samples from the second layer\n",
    "        l3: int, default 64\n",
    "            the number of output samples from the third layer\n",
    "        \"\"\"\n",
    "        super(DNNNetRegressor, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2, l3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l3, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        x = self.flatten(x)\n",
    "        return self.linear_relu_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f301740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN network for regression\n",
    "class CNNNetRegressor(nn.Module):\n",
    "    \"\"\"The class to define a 1-layer CNN Pytorch regression model\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        out: int = 16,\n",
    "        kernel_size: int = 3,\n",
    "        max_pool: int = 2,\n",
    "        l1: int = 32,\n",
    "    ) -> None:\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: int\n",
    "            the number of features for one sample\n",
    "        output_size: int\n",
    "            the final output dimension\n",
    "        out: int, default 16\n",
    "            the number of output channel for the CNN layer\n",
    "        kernel_size: int, default 3\n",
    "            the number of kernel size for the CNN layer\n",
    "        max_pool: int, default 2\n",
    "            the number of kernel size for the maxpool layer\n",
    "        l1: int, default 32\n",
    "            the number of output samples for the first linear layer\n",
    "        \"\"\"\n",
    "        super(CNNNetRegressor, self).__init__()\n",
    "        self.cnn_relu_stack = nn.Sequential(\n",
    "            nn.Conv1d(1, out, kernel_size),\n",
    "            nn.MaxPool1d(max_pool),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(out * ((input_size - kernel_size + 1) // max_pool), l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l1, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.cnn_relu_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear network for classification\n",
    "class DNNNetClassifier(nn.Module):\n",
    "    \"\"\"The class to define a 3-layer linear Pytorch classification model\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int = 1,\n",
    "        l1: int = 512,\n",
    "        l2: int = 128,\n",
    "        l3: int = 64,\n",
    "    ) -> None:\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: int\n",
    "            the input size which is the second dim from each batch\n",
    "        output_size: int, default 1\n",
    "            the dimension of the output, the default 1 means univariate\n",
    "            prediction\n",
    "        l1: int, default 512\n",
    "            the number of output samples from the first layer\n",
    "        l2: int, default 128\n",
    "            the number of output samples from the second layer\n",
    "        l3: int, default 64\n",
    "            the number of output samples from the third layer\n",
    "        \"\"\"\n",
    "        super(DNNNetClassifier, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2, l3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l3, output_size),\n",
    "        )\n",
    "        if output_size == 1:\n",
    "            self.linear_relu_stack.add_module(\"sigmoid\", nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        x = self.flatten(x)\n",
    "        return self.linear_relu_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d33df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN network for classification\n",
    "class CNNNetClassifier(nn.Module):\n",
    "    \"\"\"The class to define a 1-layer CNN Pytorch classification model\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int = 1,\n",
    "        out: int = 16,\n",
    "        kernel_size: int = 3,\n",
    "        max_pool: int = 2,\n",
    "        l1: int = 32,\n",
    "    ) -> None:\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: int\n",
    "            the number of features for one sample\n",
    "        output_size: int, default 1\n",
    "            the dimension of the output, the default 1 means univariate\n",
    "            prediction\n",
    "        out: int, default 16\n",
    "            the number of output channel for the CNN layer\n",
    "        kernel_size: int, default 3\n",
    "            the number of kernel size for the CNN layer\n",
    "        max_pool: int, default 2\n",
    "            the number of kernel size for the maxpool layer\n",
    "        l1: int, default 32\n",
    "            the number of output samples for the first linear layer\n",
    "        \"\"\"\n",
    "        super(CNNNetClassifier, self).__init__()\n",
    "        self.cnn_relu_stack = nn.Sequential(\n",
    "            nn.Conv1d(1, out, kernel_size),\n",
    "            nn.MaxPool1d(max_pool),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(out * ((input_size - kernel_size + 1) // max_pool), l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l1, output_size),\n",
    "        )\n",
    "        if output_size == 1:\n",
    "            self.cnn_relu_stack.add_module(\"sigmoid\", nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.cnn_relu_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "672bc3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_classifier(\n",
    "    config: Dict[str, Any],\n",
    "    network_name: str,\n",
    "    train_ray: ray.ObjectRef,\n",
    "    loss_fn: Callable,\n",
    "    val_ray: Optional[ray.ObjectRef] = None,\n",
    "    val_size: Optional[float] = None,\n",
    "    last_checkpoint: Optional[str] = None,\n",
    "    class_weight: bool = False,\n",
    "    num_workers: int = 0,\n",
    "    multiclass: bool = False,\n",
    "    epochs: int = 10,\n",
    "    early_stopping: Optional[EarlyStopper] = None,\n",
    "    verbose: int = 0,\n",
    "    visual_batch: int = 2000,\n",
    "    random_state: int = 0,\n",
    ") -> None:\n",
    "    \"\"\"Hyperparameter tuning for a classification PyTorch model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config: dict\n",
    "        the dictionary containing the hyperparameter grid\n",
    "    network_name: str\n",
    "        the name of the model, DNN or CNN\n",
    "    train_ray: ray.ObjectRef\n",
    "        the train data id represented by ray.ObjectRef\n",
    "    loss_fn: Callable\n",
    "        the PyTorch loss function\n",
    "    val_ray: ray.ObjectRef\n",
    "        the validation data id represented by ray.ObjectRef\n",
    "    val_size: float, default None\n",
    "        the validation data size from the train data\n",
    "    last_checkpoint: str, default None\n",
    "        the local checkpoint dir if want to continue from the last time\n",
    "    class_weight: bool, default False\n",
    "        the indicator if to use class weight when training\n",
    "    num_workers: int, default 0\n",
    "        the number of cpus when loading data\n",
    "    multiclass: bool, default False\n",
    "        the indicator if this is a multi-label classification problem\n",
    "    epochs: int, default 10\n",
    "        the number of epochs\n",
    "    early_stopping: EarlyStopper, default None\n",
    "        the instance to perform early stopping\n",
    "    verbose: int, default 0\n",
    "        0 means no logs, 1 means epoch logs, 2 means batch logs\n",
    "    visual_batch: int, default 2000\n",
    "        the number of batches when to show the on-going loss\n",
    "    random_state: int, default 0\n",
    "        the random state\n",
    "    \"\"\"\n",
    "    # build model\n",
    "    if network_name == \"DNN\":\n",
    "        network = DNNNetClassifier(\n",
    "            input_size=ray.get(train_ray)[0][0].shape[-1],\n",
    "            output_size=ray.get(train_ray)[0][1].shape[-1],\n",
    "            **config[\"model_params\"],\n",
    "        )\n",
    "    elif network_name == \"CNN\":\n",
    "        network = CNNNetClassifier(\n",
    "            input_size=ray.get(train_ray)[0][0].shape[-1],\n",
    "            output_size=ray.get(train_ray)[0][1].shape[-1],\n",
    "            **config[\"model_params\"],\n",
    "        )\n",
    "    else:\n",
    "        raise NameError(f\"Wrong network name selected: {network_name}\")\n",
    "\n",
    "    # define optimizer\n",
    "    if config[\"optimizer\"] == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(network.parameters(), lr=config[\"lr\"])\n",
    "    elif config[\"optimizer\"] == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(network.parameters(), lr=config[\"lr\"])\n",
    "    else:\n",
    "        raise NameError(f\"Wrong optimizer name selected: {config['optimizer']}\")\n",
    "\n",
    "    # load the model and optimizer from the last time\n",
    "    if last_checkpoint:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(last_checkpoint, \"checkpoint\")\n",
    "        )\n",
    "        network.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    # get training and val sets\n",
    "    assert val_ray != None or val_size != None\n",
    "    if val_ray:\n",
    "        train_subset = ray.get(train_ray)\n",
    "        val_subset = ray.get(val_ray)\n",
    "    else:\n",
    "        val_ratio = int(len(ray.get(train_ray)) * val_size)\n",
    "        train_subset, val_subset = random_split(\n",
    "            ray.get(train_ray),\n",
    "            [len(ray.get(train_ray)) - val_ratio, val_ratio],\n",
    "            generator=torch.Generator().manual_seed(random_state),\n",
    "        )\n",
    "    # real validation indices labels\n",
    "    if multiclass:\n",
    "        val_real = torch.argmax(val_subset[:][1], dim=1).numpy()\n",
    "    else:\n",
    "        val_real = val_subset[:][1].numpy().flatten()\n",
    "    # define method for metrics\n",
    "    average = \"weighted\" if multiclass else \"binary\"\n",
    "\n",
    "    # class weights\n",
    "    train_sampler = None\n",
    "    if class_weight:\n",
    "        _, counts = np.unique(train_subset[:][1].numpy(), return_counts=True)\n",
    "        class_weights = [sum(counts) / c for c in counts]\n",
    "        train_sample_weight = [\n",
    "            class_weights[int(i)] for i in train_subset[:][1].numpy().flatten()\n",
    "        ]\n",
    "        train_sampler = WeightedRandomSampler(\n",
    "            train_sample_weight,\n",
    "            len(train_sample_weight),\n",
    "            replacement=True,\n",
    "        )\n",
    "\n",
    "    # build dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_subset,\n",
    "        sampler=train_sampler,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=(train_sampler == None),\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    # training\n",
    "    size = len(train_loader)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        network.train()\n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            pred = network(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # running loss visualization\n",
    "            if batch % visual_batch == (visual_batch - 1):\n",
    "                if verbose > 1:\n",
    "                    print(\n",
    "                        (\n",
    "                            f\"epoch {epoch + 1}  batch [{batch+1:<4}/{size}]\"\n",
    "                            + f\"  loss: {(running_loss / visual_batch):.6f}\"\n",
    "                        )\n",
    "                    )\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            val_pred = network(val_subset[:][0])\n",
    "            val_loss = loss_fn(val_pred, val_subset[:][1]).item()\n",
    "        # transform for univariate or multi-class prediction\n",
    "        if multiclass:\n",
    "            val_pred = torch.argmax(val_pred, dim=1).numpy()\n",
    "        else:\n",
    "            val_pred = val_pred.detach().numpy().flatten() > 0.5\n",
    "        # metrics\n",
    "        accuracy = metrics.accuracy_score(val_real, val_pred)\n",
    "        f1 = metrics.f1_score(\n",
    "            val_real,\n",
    "            val_pred,\n",
    "            average=average,\n",
    "        )\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((network.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=val_loss, accuracy=accuracy, f1=f1)\n",
    "        if early_stopping and early_stopping(loss=val_loss):\n",
    "            logger.info(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f527ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_regressor(\n",
    "    config: Dict[str, Any],\n",
    "    network_name: str,\n",
    "    train_ray: ray.ObjectRef,\n",
    "    loss_fn: Callable,\n",
    "    val_ray: Optional[ray.ObjectRef] = None,\n",
    "    val_size: Optional[float] = None,\n",
    "    num_workers: int = 0,\n",
    "    last_checkpoint: Optional[str] = None,\n",
    "    epochs: int = 10,\n",
    "    early_stopping: Optional[EarlyStopper] = None,\n",
    "    verbose: int = 0,\n",
    "    visual_batch: int = 2000,\n",
    "    random_state: int = 0,\n",
    ") -> None:\n",
    "    \"\"\"Hyperparameter tuning for a regression PyTorch model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config: dict\n",
    "        the dictionary containing the hyperparameter grid\n",
    "    network_name: str\n",
    "        the name of the model, DNN or CNN\n",
    "    train_ray: ray.ObjectRef\n",
    "        the train data id represented by ray.ObjectRef\n",
    "    loss_fn: Callable\n",
    "        the pytorch loss function\n",
    "    val_ray: ray.ObjectRef\n",
    "        the validation data id represented by ray.ObjectRef\n",
    "    val_size: float, default None\n",
    "        the partition ratio of validation set\n",
    "    num_workers: int, default 0\n",
    "        the number of cpus when loading data\n",
    "    last_checkpoint: str, default None\n",
    "        the local checkpoint dir if want to continue from the last time\n",
    "    epochs: int, default 10\n",
    "        the number of epochs\n",
    "    early_stopping: EarlyStopper, default None\n",
    "        the instance to perform early stopping\n",
    "    verbose: int, default 0\n",
    "        the number of verbose indictor\n",
    "    visual_batch: int, default 2000\n",
    "        the number of batches when to show the on-going loss\n",
    "    random_state: int, default 0\n",
    "        the random state\n",
    "    \"\"\"\n",
    "    # build model\n",
    "    if network_name == \"DNN\":\n",
    "        network = DNNNetRegressor(\n",
    "            input_size=ray.get(train_ray)[0][0].shape[-1],\n",
    "            output_size=ray.get(train_ray)[0][1].shape[-1],\n",
    "            **config[\"model_params\"],\n",
    "        )\n",
    "    elif network_name == \"CNN\":\n",
    "        network = CNNNetRegressor(\n",
    "            input_size=ray.get(train_ray)[0][0].shape[-1],\n",
    "            output_size=ray.get(train_ray)[0][1].shape[-1],\n",
    "            **config[\"model_params\"],\n",
    "        )\n",
    "    else:\n",
    "        raise NameError(f\"Wrong model name selected: {network_name}\")\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    # load the model and optimizer from the last time\n",
    "    if last_checkpoint:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(last_checkpoint, \"checkpoint\")\n",
    "        )\n",
    "        network.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    # split into training and val sets\n",
    "    assert val_ray != None or val_size != None\n",
    "    if val_ray:\n",
    "        train_subset = ray.get(train_ray)\n",
    "        val_subset = ray.get(val_ray)\n",
    "    else:\n",
    "        val_ratio = int(len(ray.get(train_ray)) * val_size)\n",
    "        train_subset, val_subset = random_split(\n",
    "            ray.get(train_ray),\n",
    "            [len(ray.get(train_ray)) - val_ratio, val_ratio],\n",
    "            generator=torch.Generator().manual_seed(random_state),\n",
    "        )\n",
    "\n",
    "    # build dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    # training\n",
    "    size = len(train_loader)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        network.train()\n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            pred = network(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # running loss visualization\n",
    "            if batch % visual_batch == (visual_batch - 1):\n",
    "                if verbose > 1:\n",
    "                    print(\n",
    "                        (\n",
    "                            f\"epoch {epoch + 1}  batch [{batch+1:<4}/{size}]\"\n",
    "                            + f\"  loss: {(running_loss / visual_batch):.6f}\"\n",
    "                        )\n",
    "                    )\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            val_pred = network(val_subset[:][0])\n",
    "            val_loss = loss_fn(val_pred, val_subset[:][1]).item()\n",
    "        # metrics\n",
    "        mae = metrics.mean_absolute_error(\n",
    "            val_subset[:][1].numpy().flatten(),\n",
    "            val_pred.detach().numpy().flatten(),\n",
    "        )\n",
    "        mse = metrics.mean_squared_error(\n",
    "            val_subset[:][1].numpy().flatten(),\n",
    "            val_pred.detach().numpy().flatten(),\n",
    "        )\n",
    "        r2 = metrics.r2_score(\n",
    "            val_subset[:][1].numpy().flatten(),\n",
    "            val_pred.detach().numpy().flatten(),\n",
    "        )\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((network.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=val_loss, mae=mae, mse=mse, r2=r2)\n",
    "        if early_stopping and early_stopping(loss=val_loss):\n",
    "            logger.info(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    print(\"Finished Training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e7b069d7422492e5727182a0e1782c5eb57bd1588256e5d256bb3ae36d05dae9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
