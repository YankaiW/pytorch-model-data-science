{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610c331a-1654-4bb1-8819-55c3eb8944ef",
   "metadata": {},
   "source": [
    "The notebook used to store functions or PyTorch models, including, \n",
    "\n",
    "    * DNN1DNet: 1D DNN model\n",
    "    * CNN1DNet: 1D CNN model\n",
    "    * train_classifier: The function used to train a classifier based the models above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed11046-9c4c-4ddd-9db8-1935580667ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import tempfile\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import ray\n",
    "import torch\n",
    "from ray import train\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b4c09f-5e9e-420b-a88d-62083989ead6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load DNN1DNet model\n"
     ]
    }
   ],
   "source": [
    "print(\"Load DNN1DNet model\")\n",
    "\n",
    "\n",
    "# 1D DNN network\n",
    "class DNN1DNet(nn.Module):\n",
    "    \"\"\"The class to define linear Pytorch model\n",
    "\n",
    "    Note that the input data for this model can be arbitrary dimensions, but the\n",
    "    input size is better to be set up clearly before.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        hidden_layers: List[int],\n",
    "        usage: str = \"regression\",\n",
    "    ) -> None:\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: int\n",
    "            the number of dimensions of input data\n",
    "        output_size: int\n",
    "            the number of dimensions of output data\n",
    "        hidden_layers: list\n",
    "            the list containing the in/out data size in the hidden layers\n",
    "        usage: str, default \"regression\"\n",
    "            the goal of the model, regression or classification\n",
    "        \"\"\"\n",
    "        super(DNN1DNet, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential()\n",
    "        self.usage = usage\n",
    "\n",
    "        # transform into 1D\n",
    "        self.net.add_module(\"flatten\", nn.Flatten())\n",
    "        hidden_layers = [input_size] + hidden_layers + [output_size]\n",
    "\n",
    "        for idx in range(len(hidden_layers) - 1):\n",
    "            self.net.add_module(\n",
    "                f\"linear_{idx}\", nn.Linear(hidden_layers[idx], hidden_layers[idx + 1])\n",
    "            )\n",
    "            if idx < (len(hidden_layers) - 2):\n",
    "                self.net.add_module(\n",
    "                    f\"norm_{idx}\", nn.BatchNorm1d(hidden_layers[idx + 1])\n",
    "                )\n",
    "                self.net.add_module(f\"relu_{idx}\", nn.ReLU())\n",
    "        # when output size is 1, transform output to be probability\n",
    "        if hidden_layers[-1] == 1 and self.usage == \"classification\":\n",
    "            self.net.add_module(\"sigmoid\", nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            the input tensor with the shape (N_samples, n0, n1, ...), which can\n",
    "            be the arbitrary dimensions\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            the output data after model processing\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1765f01-4317-41d8-9ed0-7dbe765dbd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load CNN1DNet model\n"
     ]
    }
   ],
   "source": [
    "print(\"Load CNN1DNet model\")\n",
    "\n",
    "\n",
    "# 1D CNN network\n",
    "class CNN1DNet(nn.Module):\n",
    "    \"\"\"The class to define a CNN Pytorch model\n",
    "\n",
    "    Note that this model is strict with the input shape.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: Tuple[int],\n",
    "        output_size: int,\n",
    "        cnn_outputs: List[int],\n",
    "        kernel_sizes: int,\n",
    "        max_pools: int,\n",
    "        linear_layers: List[int],\n",
    "        usage: str = \"regression\",\n",
    "    ) -> None:\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape: tuple\n",
    "            the input shape in 2D\n",
    "        output_size: int\n",
    "            the number of dimensions of output data\n",
    "        cnn_outputs: list\n",
    "            the list of CNN layers output channel numbers\n",
    "        kernel_sizes: int\n",
    "            the  kernel size in each CNN layer\n",
    "        max_pools: int\n",
    "            the pool number in the pool layers after each CNN layer\n",
    "        linear_layers: list\n",
    "            the list containing the in/out data size in the hidden layers,\n",
    "            except the first input data size\n",
    "        usage: str, default \"regression\"\n",
    "            the goal of the model, regression or classification\n",
    "        \"\"\"\n",
    "        super(CNN1DNet, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential()\n",
    "        self.usage = usage\n",
    "        self.input_shape = input_shape\n",
    "        for idx in range(len(cnn_outputs)):\n",
    "            if idx == 0:\n",
    "                self.net.add_module(\n",
    "                    f\"cnn_{idx}\", nn.Conv1d(1, cnn_outputs[idx], kernel_sizes)\n",
    "                )\n",
    "            else:\n",
    "                self.net.add_module(\n",
    "                    f\"cnn_{idx}\",\n",
    "                    nn.Conv1d(cnn_outputs[idx - 1], cnn_outputs[idx], kernel_sizes),\n",
    "                )\n",
    "            self.net.add_module(f\"norm_{idx}\", nn.BatchNorm1d(cnn_outputs[idx]))\n",
    "            self.net.add_module(f\"relu_{idx}\", nn.ReLU())\n",
    "            self.net.add_module(f\"maxpool_{idx}\", nn.MaxPool1d(max_pools))\n",
    "            self.input_shape = (\n",
    "                cnn_outputs[idx],\n",
    "                (self.input_shape[-1] - kernel_sizes + 1) // max_pools,\n",
    "            )\n",
    "\n",
    "        # fully connection layer\n",
    "        self.net.add_module(\"fully_connection\", nn.Flatten())\n",
    "        linear_layers = (\n",
    "            [self.input_shape[0] * self.input_shape[1]] + linear_layers + [output_size]\n",
    "        )\n",
    "        for idx in range(len(linear_layers) - 1):\n",
    "            self.net.add_module(\n",
    "                f\"linear_{idx}\", nn.Linear(linear_layers[idx], linear_layers[idx + 1])\n",
    "            )\n",
    "            if idx < (len(linear_layers) - 2):\n",
    "                self.net.add_module(\n",
    "                    f\"linear_norm_{idx}\", nn.BatchNorm1d(linear_layers[idx + 1])\n",
    "                )\n",
    "                self.net.add_module(f\"linear_relu_{idx}\", nn.ReLU())\n",
    "\n",
    "        # when output size is 1, transform output to be probability\n",
    "        if linear_layers[-1] == 1 and self.usage == \"classification\":\n",
    "            self.net.add_module(\"sigmoid\", nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            the input feature data, with the shape (N_samples, channels, dimension)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            the output data after model processing\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2492a501-9140-4310-ad65-547ba3c04fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load classifier train function\n"
     ]
    }
   ],
   "source": [
    "print(\"Load classifier train function\")\n",
    "\n",
    "\n",
    "def train_classifier(\n",
    "    config: Dict[str, Any],\n",
    "    network_name: str,\n",
    "    train_ray: ray.ObjectRef,\n",
    "    loss_fn: Callable,\n",
    "    val_ray: Optional[ray.ObjectRef] = None,\n",
    "    val_size: Optional[float] = None,\n",
    "    last_checkpoint: Optional[str] = None,\n",
    "    class_weight: bool = False,\n",
    "    num_workers: int = 0,\n",
    "    multiclass: bool = False,\n",
    "    epochs: int = 10,\n",
    "    early_stopping: int = 0,\n",
    "    verbose: int = 0,\n",
    "    visual_batch: int = 2000,\n",
    "    random_state: int = 0,\n",
    ") -> None:\n",
    "    \"\"\"Hyperparameter tuning for a classification PyTorch model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config: dict\n",
    "        the dictionary containing the hyperparameter grid\n",
    "    network_name: str\n",
    "        the name of the model, DNN or CNN\n",
    "    train_ray: ray.ObjectRef\n",
    "        the train data id represented by ray.ObjectRef\n",
    "    loss_fn: Callable\n",
    "        the PyTorch loss function\n",
    "    val_ray: ray.ObjectRef\n",
    "        the validation data id represented by ray.ObjectRef\n",
    "    val_size: float, default None\n",
    "        the validation data size from the train data\n",
    "    last_checkpoint: str, default None\n",
    "        the local checkpoint dir if want to continue from the last time\n",
    "    class_weight: bool, default False\n",
    "        the indicator if to use class weight when training\n",
    "    num_workers: int, default 0\n",
    "        the number of cpus when loading data\n",
    "    multiclass: bool, default False\n",
    "        the indicator if this is a multi-label classification problem\n",
    "    epochs: int, default 10\n",
    "        the number of epochs\n",
    "    early_stopping: int, default 0\n",
    "        the number of patience for early stopping, the default 0 means no early\n",
    "        stopper applied\n",
    "    verbose: int, default 0\n",
    "        0 means no logs, 1 means epoch logs, 2 means batch logs\n",
    "    visual_batch: int, default 2000\n",
    "        the number of batches when to show the on-going loss\n",
    "    random_state: int, default 0\n",
    "        the random state\n",
    "    \"\"\"\n",
    "    # build model\n",
    "    if network_name == \"DNN1DNet\":\n",
    "        network = DNN1DNet(\n",
    "            usage=\"classification\",\n",
    "            input_size=ray.get(train_ray)[0][0].shape[-1],\n",
    "            output_size=(torch.max(ray.get(train_ray)[:][1]).item() + 1),\n",
    "            **config[\"model_parameters\"],\n",
    "        )\n",
    "    elif network_name == \"CNN1DNet\":\n",
    "        network = CNN1DNet(\n",
    "            usage=\"classification\",\n",
    "            input_shape=(\n",
    "                ray.get(train_ray)[0][0].shape[-2],\n",
    "                ray.get(train_ray)[0][0].shape[-1],\n",
    "            ),\n",
    "            output_size=(torch.max(ray.get(train_ray)[:][1]).item() + 1),\n",
    "            **config[\"model_parameters\"],\n",
    "        )\n",
    "    else:\n",
    "        raise NameError(f\"Invalid network name: {network_name}\")\n",
    "\n",
    "    # define optimizer\n",
    "    if config[\"optimizer\"] == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(network.parameters(), lr=config[\"lr\"])\n",
    "    elif config[\"optimizer\"] == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(network.parameters(), lr=config[\"lr\"])\n",
    "    else:\n",
    "        raise NameError(f\"Invalid optimizer name: {config['optimizer']}\")\n",
    "\n",
    "    # load the model and optimizer from the last time\n",
    "    if last_checkpoint:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(last_checkpoint, \"checkpoint\")\n",
    "        )\n",
    "        network.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    # get training and val sets\n",
    "    assert val_ray != None or val_size != None, \"No available validation data.\"\n",
    "    if val_ray:\n",
    "        train_subset = ray.get(train_ray)\n",
    "        val_subset = ray.get(val_ray)\n",
    "    else:\n",
    "        val_ratio = int(len(ray.get(train_ray)) * val_size)\n",
    "        train_subset, val_subset = data.random_split(\n",
    "            ray.get(train_ray),\n",
    "            [len(ray.get(train_ray)) - val_ratio, val_ratio],\n",
    "            generator=torch.Generator().manual_seed(random_state),\n",
    "        )\n",
    "\n",
    "    # define method for metrics\n",
    "    average = \"weighted\" if multiclass else \"binary\"\n",
    "\n",
    "    # class weights\n",
    "    train_sampler = None\n",
    "    if class_weight:\n",
    "        _, counts = np.unique(train_subset[:][1].numpy(), return_counts=True)\n",
    "        class_weights = [sum(counts) / c for c in counts]\n",
    "        train_sample_weight = [\n",
    "            class_weights[int(i)] for i in train_subset[:][1].numpy().flatten()\n",
    "        ]\n",
    "        train_sampler = data.WeightedRandomSampler(\n",
    "            train_sample_weight,\n",
    "            len(train_sample_weight),\n",
    "            replacement=True,\n",
    "        )\n",
    "\n",
    "    # build dataloaders\n",
    "    train_loader = data.DataLoader(\n",
    "        train_subset,\n",
    "        sampler=train_sampler,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=(train_sampler == None),\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    # check early stopping\n",
    "    if early_stopping > 0:\n",
    "        early_stopper = EarlyStopper(patience=early_stopping)\n",
    "\n",
    "    # training\n",
    "    size = len(train_loader)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        network.train()\n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            pred = network(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # running loss visualization\n",
    "            if batch % visual_batch == (visual_batch - 1):\n",
    "                if verbose > 1:\n",
    "                    print(\n",
    "                        (\n",
    "                            f\"epoch {epoch + 1}  batch [{batch+1:<4}/{size}]\"\n",
    "                            + f\"  loss: {(running_loss / visual_batch):.6f}\"\n",
    "                        )\n",
    "                    )\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # validation\n",
    "        network.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = network(val_subset[:][0])\n",
    "            val_loss = loss_fn(val_pred, val_subset[:][1]).item()\n",
    "        # transform for univariate or multi-class prediction\n",
    "        if multiclass:\n",
    "            val_pred = torch.argmax(val_pred, dim=1).numpy()\n",
    "        else:\n",
    "            val_pred = val_pred.detach().numpy().flatten() > 0.5\n",
    "        # metrics\n",
    "        accuracy = metrics.accuracy_score(val_subset[:][1].numpy(), val_pred)\n",
    "        f1 = metrics.f1_score(\n",
    "            val_subset[:][1].numpy(),\n",
    "            val_pred,\n",
    "            average=average,\n",
    "        )\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tempdir:\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state\": network.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                },\n",
    "                os.path.join(tempdir, \"checkpoint.pt\"),\n",
    "            )\n",
    "            train.report(\n",
    "                {\"loss\": val_loss, \"accuracy\": accuracy, \"f1\": f1},\n",
    "                checkpoint=train.Checkpoint.from_directory(tempdir),\n",
    "            )\n",
    "\n",
    "        if early_stopping > 0 and early_stopper(loss=val_loss):\n",
    "            logger.info(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    logger.info(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf9d74d-1ab9-4b17-80e2-cd882583b95a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
