{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ec4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import torch\n",
    "from torch import nn\n",
    "from ray import tune\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14888d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear network for regression\n",
    "class DNNNetRegressor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        l1: int = 512,\n",
    "        l2: int = 128,\n",
    "        l3: int = 64,\n",
    "    ) -> None:\n",
    "        super(DNNNetRegressor, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2, l3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l3, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.linear_relu_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear network for classification\n",
    "class DNNNetClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        l1: int = 512,\n",
    "        l2: int = 128,\n",
    "        l3: int = 64,\n",
    "    ) -> None:\n",
    "        super(DNNNetClassifier, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2, l3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l3, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.linear_relu_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "672bc3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning function\n",
    "def tune_classifier(\n",
    "    config: Dict[str, Any],\n",
    "    net_structure: str,\n",
    "    train_ray: ray.ObjectRef,\n",
    "    loss_fn: Any,\n",
    "    val_ray: Optional[ray.ObjectRef] = None,\n",
    "    val_size: Optional[float] = None,\n",
    "    last_checkpoint: Optional[str] = None,\n",
    "    class_weight: bool = False,\n",
    "    epochs: int = 10,\n",
    "    verbose: int = 0,\n",
    "    random_state: int = 0,\n",
    ") -> None:\n",
    "    \"\"\"Hyperparameter tuning for a classification PyTorch model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    net_structure: str\n",
    "        the name of the model\n",
    "    config: dict\n",
    "        the dictionary containing the hyperparameter grid\n",
    "    train_ray: ray.ObjectRef\n",
    "        the train data id represented by ray.ObjectRef\n",
    "    val_ray: ray.ObjectRef\n",
    "        the validation data id represented by ray.ObjectRef\n",
    "    val_size: float, default None\n",
    "        the validation data size from the train data\n",
    "    loss_fn: Any\n",
    "        the loss function\n",
    "    last_checkpoint: str, default None\n",
    "        the local checkpoint dir if want to continue from the last time\n",
    "    class_weight: bool, default False\n",
    "        the indicator if to use class weight when training\n",
    "    epochs: int, default 10\n",
    "        the number of epochs\n",
    "    verbose: int, default 0\n",
    "        the number of verbose indicator\n",
    "    random_state: int, default 0\n",
    "        the random state\n",
    "    \"\"\"\n",
    "    # build model\n",
    "    if net_structure == \"DNN\":\n",
    "        network = DNNNetClassifier(\n",
    "            input_size=ray.get(train_ray)[0][0].shape[0],\n",
    "            **config[\"model_params\"],\n",
    "        )\n",
    "    else:\n",
    "        raise NameError(\"Wrong network name selected: \" + net_structure)\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    # load the model and optimizer from the last time\n",
    "    if last_checkpoint:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(last_checkpoint, \"checkpoint\")\n",
    "        )\n",
    "        network.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    # get training and val sets\n",
    "    assert val_ray != None or val_size != None\n",
    "    if val_ray:\n",
    "        train_subset = ray.get(train_ray)\n",
    "        val_subset = ray.get(val_ray)\n",
    "    else:\n",
    "        val_ratio = int(len(ray.get(train_ray)) * val_size)\n",
    "        train_subset, val_subset = random_split(\n",
    "            ray.get(train_ray),\n",
    "            [len(ray.get(train_ray)) - val_ratio, val_ratio],\n",
    "            generator=torch.Generator().manual_seed(random_state),\n",
    "        )\n",
    "\n",
    "    # class weights\n",
    "    train_sampler = None\n",
    "    if class_weight:\n",
    "        _, counts = np.unique(train_subset[:][1].numpy(), return_counts=True)\n",
    "        class_weights = [sum(counts) / c for c in counts]\n",
    "        train_sample_weight = [\n",
    "            class_weights[int(i)] for i in train_subset[:][1].numpy().flatten()\n",
    "        ]\n",
    "        train_sampler = WeightedRandomSampler(\n",
    "            train_sample_weight,\n",
    "            len(train_sample_weight),\n",
    "            replacement=True,\n",
    "        )\n",
    "\n",
    "    # build dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_subset,\n",
    "        sampler=train_sampler,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=(train_sampler == None),\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # training\n",
    "    for epoch in range(epochs):\n",
    "        size = len(train_subset)\n",
    "        running_loss = 0.0\n",
    "        network.train()\n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            pred = network(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # running loss visualization\n",
    "            if batch % 2000 == 1999:\n",
    "                if verbose > 0:\n",
    "                    print(\n",
    "                        (\n",
    "                            f\"loss: {(running_loss / 2000):.6f} \"\n",
    "                            + f\"[{(batch+1)*len(X)}/{size}]\"\n",
    "                        )\n",
    "                    )\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            val_pred = network(val_loader.dataset[:][0])\n",
    "            val_loss = loss_fn(val_pred, val_loader.dataset[:][1]).item()\n",
    "        # metrics\n",
    "        precision = metrics.precision_score(\n",
    "            val_loader.dataset[:][1].numpy().flatten(),\n",
    "            val_pred.detach().numpy().flatten() > 0.5,\n",
    "        )\n",
    "        recall = metrics.recall_score(\n",
    "            val_loader.dataset[:][1].numpy().flatten(),\n",
    "            val_pred.detach().numpy().flatten() > 0.5,\n",
    "        )\n",
    "        f1 = metrics.f1_score(\n",
    "            val_loader.dataset[:][1].numpy().flatten(),\n",
    "            val_pred.detach().numpy().flatten() > 0.5,\n",
    "        )\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((network.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=val_loss, precision=precision, recall=recall, f1=f1)\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f527ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_regressor(\n",
    "    config: Dict[str, Any],\n",
    "    net_structure: str,\n",
    "    train_ray: ray.ObjectRef,\n",
    "    loss_fn: Any,\n",
    "    val_ray: Optional[ray.ObjectRef] = None,\n",
    "    val_size: Optional[float] = None,\n",
    "    last_checkpoint: Optional[str] = None,\n",
    "    epochs: int = 10,\n",
    "    verbose: int = 0,\n",
    "    random_state: int = 0,\n",
    ") -> None:\n",
    "    \"\"\"Hyperparameter tuning for a regression PyTorch model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config: dict\n",
    "        the dictionary containing the hyperparameter grid\n",
    "    net_structure: str\n",
    "        the name of the model\n",
    "    train_ray: ray.ObjectRef\n",
    "        the train data id represented by ray.ObjectRef\n",
    "    loss_fn:\n",
    "        the pytorch loss function\n",
    "    val_ray: ray.ObjectRef\n",
    "        the validation data id represented by ray.ObjectRef\n",
    "    val_size: float, default None\n",
    "        the partition ratio of validation set\n",
    "    last_checkpoint: str, default None\n",
    "        the local checkpoint dir if want to continue from the last time\n",
    "    epochs: int, default 10\n",
    "        the number of epochs\n",
    "    verbose: int, default 0\n",
    "        the number of verbose indictor\n",
    "    random_state: int, default 0\n",
    "        the random state\n",
    "    \"\"\"\n",
    "    # build model\n",
    "    if net_structure == \"DNN\":\n",
    "        network = DNNNetRegressor(\n",
    "            input_size=ray.get(train_ray)[0][0].shape[0], **config[\"model_params\"]\n",
    "        )\n",
    "    else:\n",
    "        raise NameError(\"Wrong model name selected: \" + net_structure)\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    # load the model and optimizer from the last time\n",
    "    if last_checkpoint:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(last_checkpoint, \"checkpoint\")\n",
    "        )\n",
    "        network.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    # split into training and val sets\n",
    "    assert val_ray != None or val_size != None\n",
    "    if val_ray:\n",
    "        train_subset = ray.get(train_ray)\n",
    "        val_subset = ray.get(val_ray)\n",
    "    else:\n",
    "        val_ratio = int(len(ray.get(train_ray)) * val_size)\n",
    "        train_subset, val_subset = random_split(\n",
    "            ray.get(train_ray),\n",
    "            [len(ray.get(train_ray)) - val_ratio, val_ratio],\n",
    "            generator=torch.Generator().manual_seed(random_state),\n",
    "        )\n",
    "\n",
    "    # build dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # training\n",
    "    for epoch in range(epochs):\n",
    "        size = len(train_subset)\n",
    "        running_loss = 0.0\n",
    "        network.train()\n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            pred = network(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # running loss visualization\n",
    "            if batch % 2000 == 1999:\n",
    "                if verbose > 0:\n",
    "                    print(\n",
    "                        (\n",
    "                            f\"loss: {(running_loss / 2000):.6f} \"\n",
    "                            + f\"[{(batch+1) * len(X)}/{size}]\"\n",
    "                        )\n",
    "                    )\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            val_pred = network(val_loader.dataset[:][0])\n",
    "            val_loss = loss_fn(val_pred, val_loader.dataset[:][1]).item()\n",
    "        # metrics\n",
    "        mae = metrics.mean_absolute_error(\n",
    "            val_loader.dataset[:][1].numpy().flatten(),\n",
    "            val_pred.detach().numpy().flatten(),\n",
    "        )\n",
    "        mse = metrics.mean_squared_error(\n",
    "            val_loader.dataset[:][1].numpy().flatten(),\n",
    "            val_pred.detach().numpy().flatten(),\n",
    "        )\n",
    "        r2 = metrics.r2_score(\n",
    "            val_loader.dataset[:][1].numpy().flatten(),\n",
    "            val_pred.detach().numpy().flatten(),\n",
    "        )\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((network.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=val_loss, mae=mae, mse=mse, r2=r2)\n",
    "    print(\"Finished Training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e7b069d7422492e5727182a0e1782c5eb57bd1588256e5d256bb3ae36d05dae9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
